{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qin/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 句子长度，需要跟模型训练时一致\n",
    "max_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入tokenizer\n",
    "json_file = open('token_config.json','r',encoding='utf-8')\n",
    "token_config = json.load(json_file)\n",
    "tokenizer = tokenizer_from_json(token_config)\n",
    "# 获得字典，键为字，值为编号\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入模型\n",
    "model = load_model('lstm_tag.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 载入数据集做处理主要是为了计算状态转移概率\n",
    "# 读入数据\n",
    "text = open('msr_train.txt', encoding='gb18030').read()\n",
    "# 根据换行符切分数据\n",
    "text = text.split('\\n')\n",
    "\n",
    "# 得到所有的数据和标签\n",
    "def get_data(s):\n",
    "    # 匹配(.)/(.)格式的数据\n",
    "    s = re.findall('(.)/(.)', s)\n",
    "    if s:\n",
    "        s = np.array(s)\n",
    "        # 返回数据和标签，0为数据，1为标签\n",
    "        return s[:,0],s[:,1]\n",
    "\n",
    "# 数据\n",
    "data = []\n",
    "# 标签\n",
    "label = []\n",
    "# 循环每个句子\n",
    "for s in text:\n",
    "    # 分离文字和标签\n",
    "    d = get_data(s)\n",
    "    if d:\n",
    "        # 0为数据\n",
    "        data.append(d[0])\n",
    "        # 1为标签\n",
    "        label.append(d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts二维数据，一行一个句子\n",
    "# 比如ngrams(texts,2,2)，只计算2-grams\n",
    "# 比如ngrams(texts,2,4)，计算2-grams，3-grams，4-grams\n",
    "def ngrams(texts, MIN_N, MAX_N):\n",
    "    # 定义空字典记录\n",
    "    ngrams_dict = {}\n",
    "    # 循环每一个句子\n",
    "    for tokens in texts:\n",
    "        # 计算一个句子token数量\n",
    "        n_tokens = len(tokens)\n",
    "        # 词汇组合统计\n",
    "        for i in range(n_tokens):\n",
    "            for j in range(i+MIN_N, min(n_tokens, i+MAX_N)+1):\n",
    "                # 词汇组合list转字符串\n",
    "                temp = ''.join(tokens[i:j])\n",
    "                # 字典计数加一\n",
    "                ngrams_dict[temp] = ngrams_dict.get(temp, 0) + 1\n",
    "    # 返回字典\n",
    "    return ngrams_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sb': 600115, 'be': 1039906, 'es': 659674, 'ss': 427204, 'bm': 215149, 'me': 215149, 'mm': 211874, 'eb': 594480}\n"
     ]
    }
   ],
   "source": [
    "# 统计状态转移次数\n",
    "ngrams_dict = ngrams(label,2,2)\n",
    "print(ngrams_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算状态转移总次数\n",
    "sum_num = 0\n",
    "for value in ngrams_dict.values():\n",
    "    sum_num = sum_num + value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算状态转移概率\n",
    "p_sb = ngrams_dict['sb']/sum_num\n",
    "p_be = ngrams_dict['be']/sum_num\n",
    "p_es = ngrams_dict['es']/sum_num\n",
    "p_ss = ngrams_dict['ss']/sum_num\n",
    "p_bm = ngrams_dict['bm']/sum_num\n",
    "p_me = ngrams_dict['me']/sum_num\n",
    "p_mm = ngrams_dict['mm']/sum_num\n",
    "p_eb = ngrams_dict['eb']/sum_num\n",
    "# p_oo用于表示不可能的转移，-np.inf负无穷\n",
    "p_oo = -np.inf\n",
    "\n",
    "# 使用条件随机场CRF来计算转移矩阵有可能效果会更好\n",
    "# 这里我们用简单的二元模型来定义状态转移矩阵\n",
    "# oo,os,ob,om,oe,\n",
    "# so,ss,sb,sm,se\n",
    "# bo,bs,bb,bm,be\n",
    "# mo,ms,mb,mm,me\n",
    "# eo,es,eb,em,ee\n",
    "# 其中sm,se,bs,bb,ms,mb,em,ee这几个状态转移是不存在的\n",
    "# o为填充状态，跟o相关的转移也都不需要考虑\n",
    "transition_params = [[p_oo,p_oo,p_oo,p_oo,p_oo],\n",
    "                     [p_oo,p_ss,p_sb,p_oo,p_oo],\n",
    "                     [p_oo,p_oo,p_oo,p_bm,p_be],\n",
    "                     [p_oo,p_oo,p_oo,p_mm,p_me],\n",
    "                     [p_oo,p_es,p_eb,p_oo,p_oo]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 维特比算法\n",
    "def viterbi_decode(sequence, transition_params):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      sequence: 一个[seq_len, num_tags]矩阵\n",
    "      transition_params: 一个[num_tags, num_tags]矩阵\n",
    "    Returns:\n",
    "      viterbi: 一个[seq_len]序列\n",
    "    \"\"\"\n",
    "    # 假设状态转移共有num_tags种状态\n",
    "    # 创建一个跟sequence相同形状的网格\n",
    "    score = np.zeros_like(sequence)\n",
    "    # 创建一个跟sequence相同形状的path，用于记录路径\n",
    "    path = np.zeros_like(sequence, dtype=np.int32)\n",
    "    # 起始分数\n",
    "    score[0] = sequence[0]\n",
    "    for t in range(1, sequence.shape[0]):\n",
    "        # t-1时刻score得分加上trans分数，得到下一时刻所有状态转移[num_tags, num_tags]的得分\n",
    "        T = np.expand_dims(score[t - 1], 1) + transition_params\n",
    "        # t时刻score = 计算每个状态转移的最大得分 + 下个序列预测得分\n",
    "        score[t] = np.max(T, 0) + sequence[t] \n",
    "        # 记录每个状态转移的最大得分所在位置 \n",
    "        path[t] = np.argmax(T, 0)\n",
    "    # score[-1]为最后得到的num_tags种状态得分\n",
    "    # np.argmax(score[-1])找到最高分数所在位置\n",
    "    viterbi = [np.argmax(score[-1])]\n",
    "    # 回头确定来的路径，相当于知道最高分以后从后往前走\n",
    "    for p in reversed(path[1:]):\n",
    "        viterbi.append(p[viterbi[-1]])\n",
    "    # 反转viterbi列表，把viterbi变成正向路径\n",
    "    viterbi.reverse()\n",
    "    # 计算最大得分，如果需要可以return\n",
    "    # viterbi_score = np.max(score[-1])\n",
    "    return viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小句分词函数\n",
    "def cut(sentence):\n",
    "    # 如果句子大于最大长度，只取max_length个词\n",
    "    if len(sentence) >= max_length:\n",
    "        seq = sentence[:max_length]\n",
    "    # 如果不足max_length，则填充\n",
    "    else:\n",
    "        seq = []\n",
    "        for s in sentence:\n",
    "            try:\n",
    "                # 在字典里查询编号\n",
    "                seq.append(word_index[s])\n",
    "            except:\n",
    "                # 如果不在字典里填充0\n",
    "                seq.append(0)\n",
    "        seq = seq + [0]*(max_length-len(sentence))\n",
    "    # 获得预测结果，shape(32,5)\n",
    "    preds = model.predict([seq])[0]\n",
    "    # 维特比算法\n",
    "    viterbi = viterbi_decode(preds, transition_params)\n",
    "    # 只保留跟句子相同长度的分词标注\n",
    "    y = viterbi[:len(sentence)]\n",
    "    # 分词\n",
    "    words = []\n",
    "    for i in range(len(sentence)):\n",
    "        # 如果标签为s或b，append到结果的list中\n",
    "        if y[i] in [1, 2]:\n",
    "            words.append(sentence[i])\n",
    "        else:\n",
    "        # 如果标签为m或e，在list最后一个元素中追加内容\n",
    "            words[-1] += sentence[i]\n",
    "    return  words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据符号断句\n",
    "cuts = re.compile(u'([\\da-zA-Z ]+)|[。，、？！\\.\\?,!()（）]')\n",
    "# 先分小句，再对小句分词\n",
    "def cut_word(s):\n",
    "    result = []\n",
    "    # 指针设置为0\n",
    "    i = 0\n",
    "    # 根据符号断句\n",
    "    for c in cuts.finditer(s):\n",
    "        # 对符号前的部分分词\n",
    "        result.extend(cut(s[i:c.start()]))\n",
    "        # 加入符号\n",
    "        result.append(s[c.start():c.end()])\n",
    "        # 移动指针到符号后面\n",
    "        i = c.end()\n",
    "    # 对最后的部分进行分词\n",
    "    result.extend(cut(s[i:]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['针对', '新冠', '病毒', '感染', '，', '要', '做好', '“', '早', '发现', '、', '早', '报告', '、', '早', '隔离', '、', '早', '治疗', '”', '，', '及时', '给予', '临床', '治疗', '的', '措施', '。']\n"
     ]
    }
   ],
   "source": [
    "print(cut_word('针对新冠病毒感染，要做好“早发现、早报告、早隔离、早治疗”，及时给予临床治疗的措施。'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['广义', '相对论', '是', '描写', '物质', '间', '引力', '相互', '作用', '的', '理论']\n"
     ]
    }
   ],
   "source": [
    "print(cut_word('广义相对论是描写物质间引力相互作用的理论'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['阿尔法围棋', '（', 'AlphaGo', '）', '是', '第一个', '击败', '人类', '职业', '围棋', '选手', '、', '第一个', '战胜', '围棋', '世界', '冠军', '的', '人工', '智能', '，', '是', '谷歌', '（', 'Google', '）', '旗', '下', 'DeepMind', '公司', '戴密斯·哈萨比斯', '领衔', '的', '团队', '开发', '。']\n"
     ]
    }
   ],
   "source": [
    "print(cut_word('阿尔法围棋（AlphaGo）是第一个击败人类职业围棋选手、第一个战胜围棋世界冠军的人工智能，是谷歌（Google）旗下DeepMind公司戴密斯·哈萨比斯领衔的团队开发。'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
